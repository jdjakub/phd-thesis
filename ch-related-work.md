\hypertarget{ch-related-work}{%
\chapter{Related Work}\label{ch-related-work}}
In the preceding chapters, we referenced programming systems and research literature that were directly relevant to the topics we were discussing. At this point, having presented BootstrapLab and evaluated it according to the Technical Dimensions, we can situate our contributions in the setting of more general related work. We will cover the research group from which many of our influences originate, work relating to each of the Three Properties, and the study of programming systems. Note that some references may be repeated from earlier for the sake of completeness.

# STEPS and the Legacy of VPRI
The \ac{COLA} system design\ \parencite{COLAs}, from which we have drawn the most in this work, emerged from the now-retired Viewpoints Research Institute (VPRI). VPRI aimed at creating "fundamentally new computing technologies", which is particularly visible throughout the 6-year project known as "STEPS towards a new computing" \parencite{Steps08,Steps09,Steps10,Steps11,Steps12}. The aim was to fully replicate a familiar graphical end-user operating system, with applications, in under 20,000 total lines of code. Such an ambitious goal provided the constraint needed to force innovation in distinguishing essential and accidental complexity and ways to reduce the latter. Innovations included the widespread use of domain-specific languages supported by OMeta\ \parencite{OMeta} and investment in highly flexible core abstractions as evidenced by \ac{COLA}'s object and composition models \parencite{OROM,OECM}.

Two of our Three Properties, Self-Sustainability and Notational Freedom, recur as themes in the STEPS work.^[Explicit Structure is absent, but this is to be expected owing to its niche status; see Section\ \ref{structure-editing-and-its-variations}.] Self-Sustainability is exhibited by \ac{COLA}. Mood-Specific Languages in \ac{COLA} and those supported by OMeta demonstrate what we called *syntactic freedom* in Section\ \ref{syntactic-freedom}. The Gezira\ \parencite{Gezira} and Nile\ \parencite{Nile} projects utilise a custom mathematical syntax, taking advantage of Unicode characters for expressing graphics code that is cumbersome in ordinary languages. While this does not amount to Syntactic Freedom, it is a good example of the sort of thing that Syntactic Freedom enables; we can expect more innovations like this only if it is not too difficult to deploy a custom syntax once one has been designed.

# Self-Sustainability and its Theory
The only name we are aware of for the concept we called Self-Sustain\-ability is "self-sustaining", seen in the two workshops on such systems \parencite{SSS-08,SSS-10} which featured the \ac{COLA} work. We derived our term "Self-Sustainability" to be able to refer to a property that can be present or absent in programming systems. We have referred to Self-Sustain*able* systems rather than Self-Sustain*ing* systems for consistency with this.

Self-Sustainability appears in related work as Smalltalk variants. Much of the STEPS work took place via the Squeak^[\url{https://squeak.org/}] variant, of which Pharo^[\url{https://pharo.org/}] is a descendant. Glamorous Toolkit^[\url{https://gtoolkit.com/}] is a "moldable development environment" built in Pharo. The Lively Kernel^[\url{https://lively-kernel.org/}] is a Web implementation of a Smalltalk-like environment; Fizzygum^[\url{http://fizzygum.org/}] is similar.

The problem with all of these, as regards our goals in this dissertation, is that they are all complicated software systems, with their own histories, made to be practically useful to researchers or industry. As such, knowledge of the principles and tricks for *implementing* these systems is sequestered away in the practical experience of their developers and not written down in a discoverable location. Furthermore, it would be difficult to separate knowledge about the property we are interested in (Self-Sustainability) from the various other aspects of the implementation of these systems (such as useful libraries or optimisations).

These facts made it clear to us that we would be best equipped to understand Self-Sustainability by trying to achieve it ourselves in a minimal context with minimal distractions. The related systems are self-sustainable in order to be useful for certain communities; BootstrapLab aims for Self-Sustainability to better understand it (along with the other two Properties).

We are only aware of a few sources that aim at a similar goal of understanding. The "Meta-Helix" approach of\ \parencite{Meta-helix} is intended to reduce confusion when implementing meta-circular Meta-Object-Protocols. As we mentioned in Section\ \ref{precursors-of-self-sustainability}, meta-circularity is a specific manifestation of Self-Sustainability. The exhaustive development of Procedural Reflection for Lisp-like languages in\ \parencite{ProcRefl} is helpful for its philosophical rigour, \eg{} the use-mention distinction and careful precision of terminology. The process described in\ \parencite{Bootfrom0} is a parallel of what we did with BootstrapLab in the restricted context of batch-mode interpreters of text strings. The introductory sections of\ \parencite{COLAs} and\ \parencite{OROM} constitute a good explanation of Self-Sustainability and why it is desirable, as does\ \parencite{CookClay}.

# Video Games
Video games and game engines are led in the *direction* of Self-Sustainabil\-ity by their nature as highly dynamic, long-lived virtual environments. They are an example of the issues we covered in Section\ \ref{static-commitment}. The creative world-building nature of games means that requirements change more often than other types of software; development is partly a process of discovery of what the final product should be. Accordingly, it is important to rapidly prototype and iterate ideas, especially by artists or other specialists who may not be expert programmers. This incentivises ways to try out new ideas without the costly operation of restarting a large, resource intensive process or the even more costly operation of re-compiling the underlying program. It also incentivises editing tools (for game levels, internal scripting languages, or configuration) to be part of the game software itself. After development, these internal "developer tools" may either be stripped from the version shipped to customers or hidden (in which case, enterprising customers will discover them eventually).

Since games also have strong requirements for real-time performance (responsiveness to input, rendering of complex scenes, simulating physics, synchronising a shared world across the internet, and managing worlds too large to fit into memory all at once) languages like C++ are a standard choice for implementation. However, the default data structuring mechanisms of these languages (such as C++ classes) must necessarily be avoided for directly modelling highly dynamic relationships between objects in the simulated world. A C++ class promises a *static commitment* to *always* contain its listed member variables of the specified types, member functions of the specified signatures, and to *always* remain in any inheritance relationships with other classes. This rules out a majority of the dynamic change that is inherent to the behaviour of a game and its development process. For this reason, standard game programming patterns\ \parencite{GPP} build infrastructure to work around this and support the modelling of objects whose relationships and contained properties may change during run time. "Entity Component Systems" \parencite{ECS} are a widely used architecture for this purpose. The general pattern of working around static commitment is known as "Greenspun's Tenth Rule" \parencite{Greenspun10}:

> Any sufficiently complicated C or Fortran program contains an ad-hoc, informally-specified, bug ridden, slow implementation of half of CommonLisp.

# Novel Notations versus Notational Freedom
"Visual Programming" contains many examples of custom notations for program code and data. Sketchpad\ \parencite{Sketchpad} is an early, influential example of diagrammatic notation augmented with the dynamic capabilities of computation. The Apparatus editor^[\url{http://aprt.us/}] is a Web-based editor for dynamic graphics influenced by Sketchpad. Bret Victor's presentations\ \parencite{DDV} demonstrate programmatic graphics based on direct manipulation instead of textual code. Data is represented in Boxer\ \parencite{Boxer} and Forms/3\ \parencite{Forms3} as named, nested boxes; in Boxer, programs reside in textual code boxes. Programming By Example and Programming By Demonstration\ \parencite{WWID,YWIMC} involve custom notations designed for either representing program structures or for supplying example input-output pairs from which to infer general behaviour. Sketch-n-Sketch\ \parencite{SnS} uses a textual and graphical notation that are synchronised with each other.

There are many more examples of custom programming\ notations and interfaces \parencite{VPcodex,VPsurvey,GalleryUIs}. Despite the abundance of earnest attempts at general-purpose or special-purpose notations, programming is still mostly performed via plain text. This is understandable given that much of the cited research is experimental and that programming infrastructure (editors, compilers, version control etc.) only supports plain text. There is also a potential failure mode of imposing a single notation for all purposes and the fact that different people have preferences about the tools they work with.

Thus, while we respect the effort invested in Visual Programming and custom notations and wish these efforts success, we avoid the conclusion that there is an optimal notation or set of notations (across users and situations) waiting to be found. Instead, we see as-yet unrealised gains in supporting and encouraging the *ad-hoc* use of custom notations on an opportunistic basis, wherever the user judges them to be most helpful. We observe much effort expended over the years on developing custom notations but comparatively little on enabling them to be used together *à la carte*, which is why we focus on Notational *Freedom.*

We see efforts towards Notational Freedom in JetBrain's MPS\ \parencite{MPS}, the Eco editor\ \parencite{Eco}, and the Mood-Specific Languages of \ac{COLA}\ \parencite{COLAs} and OMeta\ \parencite{OMeta}. Our issue with MPS is similar to what we said about Smalltalk-like systems in Section\ \ref{self-sustainability-and-its-theory}: it is impressive and useful as an industry tool in which to get things done, but its size and complexity makes it hard to learn the essential aspects of supporting Notational Freedom. Eco has the advantage of being a research project whose paper *does* cover its design and implementation, so its approach deserves a place in future work on BootstrapLab (see Section\ \ref{import-from-related-work}).

# Structure Editing and Its Variations
Explicit Structure has precedent in structure editors, projectional editors and block-based languages \ \parencite{StrucEd}, but these approaches have met many difficulties in terms of widespread adoption. Text editing and plain text formats are still entrenched as the *de facto* standard in programming. *Outside* of programming, we observe the opposite situation, which allows us to point there for intuition about why Explicit Structure is a sensible concept and could be beneficial.

For example, photo editing and vector graphics programs exist and are optimised for the types of interactions involved in those domains. Photo and vector graphics files are not required to be readable in a text editor (\ie{} we are free to distribute graphics in forms other than ASCII art) and so these domains do not suffer from the accidental complexities of text formats. If we observe that the textual syntax of programs is really a *proxy* for tree and graph structures, then this invites the investigation of the costs and benefits of treating programming structures the same way we do other types of files.

We are aware of two projects especially concerned with Explicit Structure: the Subtext programming system\ \parencite{Subtext} and the Infra\ \parencite{Infra} data interchange format. Subtext explores novel programming ideas that are only feasible from a basis of Explicit Structure, while Infra is proposed as a common format unifying text and binary data. We find these sources particularly valuable for explaining Explicit Structure and arguing its benefits.

\joel{
# Against Conventional Wisdom
A plausible hypothesis about *why* programming is the way it is---and hence why this thesis has a novel contribution to make---concerns a mismatch between programming's *military-industrial* history and its modern potential for *personal* computing. This is the logic behind the work of Kell and Basman.
\cite{Kell-os}
\cite{Externalise}
\cite{Kell-mmm}
\cite{OAP}
\cite{Entangle}
\cite{Entangle-critique}
\cite{TcherDiss}
\cite{SwStudies}
\cite{Wisdom}
\cite{Top}
}

# Programming Systems and their Analysis
Our "programming systems" approach lies between a narrow focus on programming languages and a broad focus on programming as a socio-political and cultural subject. The concept of a programming system is technical in scope, although we acknowledge the technical side often has important social implications as in the case of the "Adoptability" dimension (Section\ \ref{adoptability}). This contrasts with the more socio-political focus found in\ \textcite{TcherDiss} or in software studies\ \parencite{SwStudies}. It overlaps with Kell's conceptualisation of Unix, Smalltalk, and Operating Systems generally\ \parencite{Kell-OS}.

The distinction between more narrow _programming languages_ and broader _programming systems_ is more subtle. Richard Gabriel noted an invisible paradigm shift from the study of "systems" to the study of "languages" in computer science during the 1990s\ \parencite{PLrev}, and this observation informs our distinction here. One consequence of the change is that a *language* is often formally specified apart from any specific implementations, while *systems* resist formal specification and are often *defined by* an implementation. We recognise typical programming language implementations (\eg{} including an ordinary compiler and text editor) as a *small region* of the space of possible systems, at least as far as interaction and notations might go. Our attention is drawn to *interactive programming system* aspects of languages, such as text editing and command-line workflows.

## Programming systems research
There is renewed interest in programming systems in the form of recent non-traditional programming tools:

- Computational notebooks such as Jupyter\ \parencite{Jupyter} facilitate data analysis by combining code snippets with text and visual output, in a manner reminiscent of Literate Programming\ \parencite{LitProg}. They are backed by stateful "kernels" and used interactively.
- "Low code" end-user programming systems allow application development (mostly) through a \ac{GUI}. One example is Coda\ \parencite{CodaWeb}, which combines tables, formulas, and scripts to enable non-technical people to build "applications as documents".
- Domain-specific programming systems such as Dark\ \parencite{DarkWeb}, which claims a "holistic" programming experience for cloud API services. This includes a language, a direct manipulation editor, and near-instantaneous building and deployment.
- Even for general purpose programming with conventional tools, systems like Replit\ \parencite{ReplitWeb} have demonstrated the benefits of integrating all needed languages, tools, and user interfaces into a seamless experience, available from the browser, that requires no setup.

Research that follows the programming systems perspective can be found in a number of research venues. Those include Human-Computer Interaction conferences such as [UIST](https://uist.acm.org/)^[ACM Symposium on User Interface Software and Technology] and [VL/HCC](https://conferences.computer.org/VLHCC/)^[IEEE Symposium on Visual Languages and Human-Centric Computing]. However, work in those often emphasises the user experience over technical description. Programming systems are often presented in workshops such as [LIVE](https://liveprog.org/) and [PX](https://2021.programming-conference.org/home/px-2021)^[Programming eXperience]. However, work in those venues is often limited to the authors' individual perspectives and suffers from the aforementioned difficulty of comparing to other systems.

Concrete examples of systems were given in Section\ \ref{examples-of-programming-systems}. Recent systems which motivated some of our dimensions include Subtext\ \parencite{Subtext}, which combines code with its live execution in a single editable representation; Sketch-n-sketch\ \parencite{SnS}, which can synthesise code by direct manipulation of its outputs; Hazel\ \parencite{Hazel}, a live functional programming environment with typed holes to enable execution of incomplete or ill-typed programs; and Webstrates\ \parencite{Webstrates}, which extends Web pages with real-time sharing of state.

## Already-known characteristics
There are several existing projects identifying characteristics of programming systems. Some revolve around a single one, such as levels of liveness\ \parencite{Liveness}, or plurality and communicativity\ \parencite{Kell-C}. Others propose an entire collection. *Memory Models of Programming Languages*\ \parencite{Mem-mods} identifies the "everything is an X" metaphors underlying many programming systems; for example, the "everything is a file" of Unix and the "everything is an object" of Smalltalk. The *Design Principles of Smalltalk*\ \parencite{STdesign} documents the philosophical goals and dicta used in the design of Smalltalk; the "Gang of Four" *Design Patterns*\ \parencite{DesPats} catalogues specific implementation tactics; and the *Cognitive Dimensions of Notation*\ \parencite{CogDims} introduces a common vocabulary for software's *notational surface* and for identifying their trade-offs.

The latter two directly influence our Technical Dimensions framework. Firstly, the Cognitive Dimensions are a set of qualitative properties which can be used to analyse *notations*. We are extending this approach to the "rest" of a system, beyond its notation, with *Technical* Dimensions. Secondly, our individual dimensions naturally fall under larger *clusters* that we present in a regular format, similar to the presentation of the classic Design Patterns. As for characteristics identified by others, part of our contribution is to integrate them under a common umbrella: the existing concepts of liveness, pluralism, and uniformity metaphors ("everything is an X") become dimensions in our framework.

### Methodology
We follow the attitude of *Evaluating Programming Systems*\ \parencite{EvProgSys} in distinguishing our work from HCI methods and empirical evaluation. We are generally concerned with characteristics that are not obviously amenable to statistical analysis (\eg{} mining software repositories) or experimental methods like controlled user studies, so numerical quantities are generally not featured.

Similar development seems to be taking place in HCI research focused on user interfaces. The UIST guidelines\ \parencite{UISTAuthor} instruct authors to evaluate system contributions holistically, and the community has developed heuristics for such evaluation, such as *Evaluating User Interface Systems Research*\ \parencite{EvUISR}. Our set of dimensions offers similar heuristics for identifying interesting aspects of programming systems, though they focus more on underlying technical properties than the surface interface.

Finally, we believe that the aforementioned paradigm shift from programming systems to programming languages has hidden many ideas about programming that are worth recovering and developing further\ \parencite{ComplementaryBasic}. Thus our approach is related to the idea of _complementary science_ developed by Chang\ \parencite{Chang} in the context of history and philosophy of science. Chang argues that even in disciplines like physics, superseded or falsified theories may still contain interesting ideas worth documenting. In the field of programming, where past systems are discarded for many reasons besides empirical failure, Chang's _complementary science_ approach seems particularly suitable.
